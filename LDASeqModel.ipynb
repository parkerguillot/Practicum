{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, ldaseqmodel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import logging\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import mysql.connector\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(host=\"ba-isdsclass-dev2.lsu.edu\",\n",
    "                                user=\"aguil66\",\n",
    "                                passwd=\"msadatabase\",\n",
    "                                database=\"msa_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = (\"Select * from text order by article_date asc;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_sql(sql,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             article_url  \\\n",
      "0      https://thehill.com/blogs/twitter-room/other-n...   \n",
      "1      https://thehill.com/policy/healthcare/194204-o...   \n",
      "2      https://thehill.com/policy/healthcare/228294-o...   \n",
      "3      https://thehill.com/policy/healthcare/312343-d...   \n",
      "4      https://thehill.com/policy/healthcare/311908-h...   \n",
      "5      https://thehill.com/policy/healthcare/275147-n...   \n",
      "6      https://thehill.com/policy/healthcare/275191-g...   \n",
      "7      https://thehill.com/policy/healthcare/275205-n...   \n",
      "8      https://thehill.com/policy/healthcare/275219-s...   \n",
      "9      https://thehill.com/policy/healthcare/275273-r...   \n",
      "10     https://thehill.com/policy/healthcare/275291-s...   \n",
      "11     https://thehill.com/policy/healthcare/194272-a...   \n",
      "12     https://thehill.com/blogs/floor-action/house/1...   \n",
      "13     https://thehill.com/blogs/blog-briefing-room/n...   \n",
      "14     https://thehill.com/blogs/blog-briefing-room/n...   \n",
      "15     https://thehill.com/blogs/twitter-room/other-n...   \n",
      "16     https://thehill.com/policy/healthcare/194247-w...   \n",
      "17     https://thehill.com/policy/technology/194251-s...   \n",
      "18     https://thehill.com/policy/healthcare/194265-1...   \n",
      "19     https://thehill.com/regulation/pending-regs/22...   \n",
      "20     https://thehill.com/policy/healthcare/228387-c...   \n",
      "21     https://thehill.com/policy/healthcare/312396-p...   \n",
      "22     https://thehill.com/policy/healthcare/312401-p...   \n",
      "23     https://thehill.com/blogs/floor-action/senate/...   \n",
      "24     https://thehill.com/policy/healthcare/367072-c...   \n",
      "25     https://thehill.com/policy/healthcare/367074-h...   \n",
      "26     https://thehill.com/policy/healthcare/overnigh...   \n",
      "27     https://thehill.com/policy/healthcare/367133-c...   \n",
      "28     https://thehill.com/policy/healthcare/367035-h...   \n",
      "29     https://thehill.com/policy/healthcare/423563-t...   \n",
      "...                                                  ...   \n",
      "37646  http://thetoxicologisttoday.blogspot.com/2018/...   \n",
      "37647  http://thetoxicologisttoday.blogspot.com/2011/...   \n",
      "37648  http://thetoxicologisttoday.blogspot.com/2010/...   \n",
      "37649  http://thetoxicologisttoday.blogspot.com/2012/...   \n",
      "37650  http://thetoxicologisttoday.blogspot.com/2012/...   \n",
      "37651  http://thetoxicologisttoday.blogspot.com/2018/...   \n",
      "37652  http://thetoxicologisttoday.blogspot.com/2012/...   \n",
      "37653  http://thetoxicologisttoday.blogspot.com/2013/...   \n",
      "37654  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
      "37655  http://thetoxicologisttoday.blogspot.com/2017/...   \n",
      "37656  http://thetoxicologisttoday.blogspot.com/2012/...   \n",
      "37657  http://thetoxicologisttoday.blogspot.com/2014/...   \n",
      "37658  http://thetoxicologisttoday.blogspot.com/2012/...   \n",
      "37659  http://thetoxicologisttoday.blogspot.com/2014/...   \n",
      "37660  http://thetoxicologisttoday.blogspot.com/2017/...   \n",
      "37661  http://thetoxicologisttoday.blogspot.com/2014/...   \n",
      "37662  http://thetoxicologisttoday.blogspot.com/2016/...   \n",
      "37663  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
      "37664  http://thetoxicologisttoday.blogspot.com/2018/...   \n",
      "37665  http://thetoxicologisttoday.blogspot.com/2013/...   \n",
      "37666  http://thetoxicologisttoday.blogspot.com/2013/...   \n",
      "37667  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
      "37668  http://thetoxicologisttoday.blogspot.com/2013/...   \n",
      "37669  http://thetoxicologisttoday.blogspot.com/2018/...   \n",
      "37670  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
      "37671  http://thetoxicologisttoday.blogspot.com/2018/...   \n",
      "37672  http://thetoxicologisttoday.blogspot.com/2020/...   \n",
      "37673  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
      "37674  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
      "37675  http://thetoxicologisttoday.blogspot.com/2012/...   \n",
      "\n",
      "                      article_date  \\\n",
      "0            01/01/14 05:22 PM EST   \n",
      "1            01/01/14 06:00 AM EST   \n",
      "2            01/01/15 01:30 PM EST   \n",
      "3            01/01/17 09:39 AM EST   \n",
      "4            01/01/17 10:30 AM EST   \n",
      "5            01/02/13 02:00 PM EST   \n",
      "6            01/02/13 04:22 PM EST   \n",
      "7            01/02/13 04:53 PM EST   \n",
      "8            01/02/13 05:20 PM EST   \n",
      "9            01/02/13 09:10 PM EST   \n",
      "10           01/02/13 09:37 PM EST   \n",
      "11           01/02/14 01:00 PM EST   \n",
      "12           01/02/14 01:56 PM EST   \n",
      "13           01/02/14 02:08 PM EST   \n",
      "14           01/02/14 06:55 AM EST   \n",
      "15           01/02/14 08:59 AM EST   \n",
      "16           01/02/14 10:01 AM EST   \n",
      "17           01/02/14 10:39 AM EST   \n",
      "18           01/02/14 12:54 PM EST   \n",
      "19           01/02/15 06:09 AM EST   \n",
      "20           01/02/15 12:32 PM EST   \n",
      "21           01/02/17 02:44 PM EST   \n",
      "22           01/02/17 03:50 PM EST   \n",
      "23           01/02/17 10:54 PM EST   \n",
      "24           01/02/18 01:13 PM EST   \n",
      "25           01/02/18 01:21 PM EST   \n",
      "26           01/02/18 05:30 PM EST   \n",
      "27           01/02/18 06:26 PM EST   \n",
      "28           01/02/18 10:40 AM EST   \n",
      "29           01/02/19 03:36 PM EST   \n",
      "...                            ...   \n",
      "37646       Wednesday, 23 May 2018   \n",
      "37647    Wednesday, 24 August 2011   \n",
      "37648    Wednesday, 25 August 2010   \n",
      "37649   Wednesday, 25 January 2012   \n",
      "37650      Wednesday, 25 July 2012   \n",
      "37651      Wednesday, 25 July 2018   \n",
      "37652      Wednesday, 27 June 2012   \n",
      "37653    Wednesday, 28 August 2013   \n",
      "37654   Wednesday, 28 January 2015   \n",
      "37655      Wednesday, 28 June 2017   \n",
      "37656     Wednesday, 28 March 2012   \n",
      "37657       Wednesday, 28 May 2014   \n",
      "37658  Wednesday, 28 November 2012   \n",
      "37659   Wednesday, 29 January 2014   \n",
      "37660     Wednesday, 29 March 2017   \n",
      "37661   Wednesday, 29 October 2014   \n",
      "37662     Wednesday, 3 August 2016   \n",
      "37663  Wednesday, 30 December 2015   \n",
      "37664   Wednesday, 31 January 2018   \n",
      "37665      Wednesday, 31 July 2013   \n",
      "37666      Wednesday, 31 July 2013   \n",
      "37667   Wednesday, 4 November 2015   \n",
      "37668  Wednesday, 4 September 2013   \n",
      "37669       Wednesday, 6 June 2018   \n",
      "37670    Wednesday, 7 January 2015   \n",
      "37671   Wednesday, 7 November 2018   \n",
      "37672    Wednesday, 8 January 2020   \n",
      "37673       Wednesday, 8 July 2015   \n",
      "37674       Wednesday, 8 July 2015   \n",
      "37675        Wednesday, 9 May 2012   \n",
      "\n",
      "                                           article_title  \\\n",
      "0      Obama tweets: 'I signed the ACA for kids like ...   \n",
      "1               O-Care coverage begins, presenting tests   \n",
      "2                            ObamaCare's 2015 challenges   \n",
      "3      Incoming Dem rep: Getting rid of ObamaCare wit...   \n",
      "4             House, Senate headed for clash on Medicare   \n",
      "5                                       News bites: Done   \n",
      "6               Group cheers Obama birth-control mandate   \n",
      "7                      Nursing homes praise 'cliff' deal   \n",
      "8       Study: New abortion restrictions dropped in 2012   \n",
      "9       Rep. Lankford to lead new Oversight health panel   \n",
      "10     Sen. Kirk says stroke changed perspective on M...   \n",
      "11     Abortion-rights group finds anti-abortion regu...   \n",
      "12                House GOP takes fresh aim at ObamaCare   \n",
      "13     Poll: 59 percent report negative experience wi...   \n",
      "14     Public lacks confidence in government, poll finds   \n",
      "15           Cruz uses Dr. Evil to promote O-Care repeal   \n",
      "16     White House ‘confident’ in ObamaCare contracep...   \n",
      "17      Study: Drivers on phone twice as likely to crash   \n",
      "18     Eleven attorneys general slam Obama healthcare...   \n",
      "19                          Regulations to watch in 2015   \n",
      "20           CDC to hire lab safety chief amid criticism   \n",
      "21     Pelosi: Ball in GOP court on ObamaCare replace...   \n",
      "22           Pence to huddle with House GOP on ObamaCare   \n",
      "23        Paul: Repeal, replace ObamaCare simultaneously   \n",
      "24     Clinton: Short-term CHIP extension 'doesn't cu...   \n",
      "25     Hospital groups dig in after cuts to discount ...   \n",
      "26     Overnight Health Care: House GOP eyes entitlem...   \n",
      "27     Confirmation hearing for Trump's health secret...   \n",
      "28     House GOP whip: Entitlement reform, ObamaCare ...   \n",
      "29     Trump predicts Supreme Court will overturn Oba...   \n",
      "...                                                  ...   \n",
      "37646  \\nThe real survival rates to cancer - Part 1 o...   \n",
      "37647     \\n1st Anniversary of The Toxicologist Today!\\n   \n",
      "37648                  \\nThe Edible Amanita Muscaria?!\\n   \n",
      "37649  \\nWhat about some \"Regulatory affairs in the U...   \n",
      "37650  \\nSnapGene or the best DNA playing software yo...   \n",
      "37651  \\nThe ToxicologistToday just contributed to Av...   \n",
      "37652  \\nWhat about some \"Regulatory affairs in the U...   \n",
      "37653      \\n3rd Anniversary of The Toxicologist Today\\n   \n",
      "37654  \\nGreat science news from a recent past - Part...   \n",
      "37655                           \\nDid you know that...\\n   \n",
      "37656  \\nThe strange new craft of making life from sc...   \n",
      "37657                     \\nI am hiring... an Employer\\n   \n",
      "37658  \\nA personal farewell to Professor Keith Campb...   \n",
      "37659     \\nAn Insane swaga, an Insane Je ne sais quoi\\n   \n",
      "37660                      \\nClosure to homoSOYxuality\\n   \n",
      "37661  \\nThe stuff they say about Ebola - \"Are we all...   \n",
      "37662                   \\nAvocado for iron deficiency?\\n   \n",
      "37663       \\n2016's Miraculous Scientific Resolution \\n   \n",
      "37664                             \\nI'm writing a book\\n   \n",
      "37665  \\n1st Survival Summer Course - Size Up The Sit...   \n",
      "37666                     \\n1st Survival Summer Course\\n   \n",
      "37667  \\nWhat is the Difference Between Bio and Non-B...   \n",
      "37668  \\nTerritorial Pissings - removing cat's urine ...   \n",
      "37669  \\nThe real survival rates to cancer - Part 2 o...   \n",
      "37670  \\nGreat science news from a recent past - Part...   \n",
      "37671  \\n'Amar a diferença' - a real case of a wonder...   \n",
      "37672  \\nOn the management of Paediatric Vesicoureter...   \n",
      "37673  \\nChris Dodd's opening welcome @ LeSpar AMR Wo...   \n",
      "37674  \\nLESPAR Interdisciplinary Networking Workshop...   \n",
      "37675     \\nDiscussing PhD careers outside of academia\\n   \n",
      "\n",
      "                                            article_text  \\\n",
      "0      President Obama on Wednesday gave a shoutout t...   \n",
      "1       Today is the day for ObamaCare.President Obam...   \n",
      "2      As ObamaCare enters its fifth year, there are ...   \n",
      "3      Rep.-elect Lisa Blunt Rochester (D-Del.) said ...   \n",
      "4      Senate and House Republicans are headed for a ...   \n",
      "5      Mass. bill would license naturopathic doctorsB...   \n",
      "6      “As new healthcare plans come under the reach ...   \n",
      "7      \"This profession needs a break to return as he...   \n",
      "8      Guttmacher lamented the 2012 figure as the \"se...   \n",
      "9      The revamp gives health issues — sure to inclu...   \n",
      "10     \"I will look much more carefully at the Illino...   \n",
      "11     The number of anti-abortion laws passed in the...   \n",
      "12     The House plans to take fresh aim at ObamaCare...   \n",
      "13     Fifty-nine percent of uninsured Americans repo...   \n",
      "14     Only one in 10 people in the United States say...   \n",
      "15     Sen. Ted CruzRafael (Ted) Edward CruzGOP chair...   \n",
      "16     The White House remains “confident” that Obama...   \n",
      "17     Experienced drivers on the road are more than ...   \n",
      "18     Eleven GOP attorneys general say the Obama adm...   \n",
      "19     With President Obama determined to leave his m...   \n",
      "20     The Centers for Disease Control and Prevention...   \n",
      "21     House Minority Leader Nancy Pelosi (D-Calif.) ...   \n",
      "22     Vice President-elect Mike PenceMichael (Mike) ...   \n",
      "23     Sen. Rand PaulRandal (Rand) Howard PaulHillico...   \n",
      "24     Hillary ClintonHillary Diane Rodham ClintonWat...   \n",
      "25     Hospital groups are vowing to push forward wit...   \n",
      "26     House GOP whip: Entitlement reform, ObamaCare ...   \n",
      "27     President TrumpDonald John TrumpTrump endorses...   \n",
      "28     ObamaCare repeal and entitlement reform are at...   \n",
      "29     President TrumpDonald John TrumpTrump endorses...   \n",
      "...                                                  ...   \n",
      "37646  It is very difficult for anyone detached from ...   \n",
      "37647  Today is the 1st Anniversary of my science blo...   \n",
      "37648  As cliché as any other subject related to Toxi...   \n",
      "37649  Preface: This is the module where one gets to ...   \n",
      "37650  Here I am in a marathon of articles to the end...   \n",
      "37651  If you follow this blog you've might  have com...   \n",
      "37652  Preface: I have been absent. Loads of work in ...   \n",
      "37653  Sometimes I don't know where I find time to up...   \n",
      "37654  The saga of great science news published/broad...   \n",
      "37655  It's time for a break. It has been a loooong t...   \n",
      "37656  I was yesterday just sitting down after having...   \n",
      "37657  I will be done with my 4 years PhD in Molecula...   \n",
      "37658  A few days ago I met a friend of mine at Belle...   \n",
      "37659  I'm pretty sure most of you guys have already ...   \n",
      "37660  I have been a vegetarian for 20 years now, giv...   \n",
      "37661  Finally we reach an end, but not for the proba...   \n",
      "37662  On the 2nd of July, exactly 20 days before my ...   \n",
      "37663  Throughout 2015 the world realised yet again t...   \n",
      "37664  Seriously! No joke. To be honest if you knew m...   \n",
      "37665  If you find yourself all alone with little or ...   \n",
      "37666  A few weeks ago I had an idea, based on an app...   \n",
      "37667  When two forces in the universe secretively co...   \n",
      "37668  I recently moved to a new house to find out th...   \n",
      "37669  Following on the first part (see here) where t...   \n",
      "37670  2014 was proficient in great science. News of ...   \n",
      "37671  When I moved from Portugal to England I was ta...   \n",
      "37672  For any parent/carer dealing with a child diag...   \n",
      "37673  If you haven't read the previous post please d...   \n",
      "37674  Yesterday I attended the LESPAR Interdisciplin...   \n",
      "37675  I'll be back to work on Monday, my rib is heal...   \n",
      "\n",
      "                                                  author twitter  \\\n",
      "0                             \\n    By Mario Trujillo -            \n",
      "1                            \\n    By Jonathan Easley -            \n",
      "2                               \\n    By Sarah Ferris -            \n",
      "3                                    Mallory Shelbourne            \n",
      "4                                       Alexander Bolton           \n",
      "5                              \\n    By Elise Viebeck -            \n",
      "6                              \\n    By Elise Viebeck -            \n",
      "7                              \\n    By Elise Viebeck -            \n",
      "8                              \\n    By Elise Viebeck -            \n",
      "9                              \\n    By Elise Viebeck -            \n",
      "10                             \\n    By Elise Viebeck -            \n",
      "11                                \\n    By Blake Neff -            \n",
      "12                            \\n    By Russell Berman -            \n",
      "13                            \\n    By Mario Trujillo -            \n",
      "14                            \\n    By Rebecca Shabad -            \n",
      "15                            \\n    By Rebecca Shabad -            \n",
      "16                               \\n    By Justin Sink -            \n",
      "17                             \\n    By Julian Hattem -            \n",
      "18                            \\n    By Rebecca Shabad -            \n",
      "19                             \\n    By Lydia Wheeler -            \n",
      "20                              \\n    By Sarah Ferris -            \n",
      "21                            \\n    By Melanie Zanona -            \n",
      "22                            \\n    By Melanie Zanona -            \n",
      "23                            \\n    By Jordain Carney -            \n",
      "24                                      Nathaniel Weixel           \n",
      "25                           \\n    By Jessie Hellmann -            \n",
      "26     \\n    By Nathaniel Weixel, Peter Sullivan and ...           \n",
      "27                           \\n    By Jessie Hellmann -            \n",
      "28                           \\n    By Jessie Hellmann -            \n",
      "29                           \\n    By Jessie Hellmann -            \n",
      "...                                                  ...     ...   \n",
      "37646                                             Pudget           \n",
      "37647                                             Pudget           \n",
      "37648                                             Pudget           \n",
      "37649                                             Pudget           \n",
      "37650                                             Pudget           \n",
      "37651                                             Pudget           \n",
      "37652                                             Pudget           \n",
      "37653                                             Pudget           \n",
      "37654                                             Pudget           \n",
      "37655                                             Pudget           \n",
      "37656                                             Pudget           \n",
      "37657                                             Pudget           \n",
      "37658                                             Pudget           \n",
      "37659                                             Pudget           \n",
      "37660                                             Pudget           \n",
      "37661                                             Pudget           \n",
      "37662                                             Pudget           \n",
      "37663                                             Pudget           \n",
      "37664                                             Pudget           \n",
      "37665                                             Pudget           \n",
      "37666                                             Pudget           \n",
      "37667                                             Pudget           \n",
      "37668                                             Pudget           \n",
      "37669                                             Pudget           \n",
      "37670                                             Pudget           \n",
      "37671                                             Pudget           \n",
      "37672                                             Pudget           \n",
      "37673                                             Pudget           \n",
      "37674                                             Pudget           \n",
      "37675                                             Pudget           \n",
      "\n",
      "                timestamp  \n",
      "0     2020-02-29 15:19:44  \n",
      "1     2020-02-29 15:19:44  \n",
      "2     2020-02-29 15:14:35  \n",
      "3     2020-02-29 15:07:32  \n",
      "4     2020-02-29 15:07:31  \n",
      "5     2020-02-29 15:23:37  \n",
      "6     2020-02-29 15:23:37  \n",
      "7     2020-02-29 15:23:34  \n",
      "8     2020-02-29 15:23:34  \n",
      "9     2020-02-29 15:23:35  \n",
      "10    2020-02-29 15:23:35  \n",
      "11    2020-02-29 15:19:41  \n",
      "12    2020-02-29 15:19:41  \n",
      "13    2020-02-29 15:19:42  \n",
      "14    2020-02-29 15:19:44  \n",
      "15    2020-02-29 15:19:44  \n",
      "16    2020-02-29 15:19:44  \n",
      "17    2020-02-29 15:19:41  \n",
      "18    2020-02-29 15:19:42  \n",
      "19    2020-02-29 15:14:35  \n",
      "20    2020-02-29 15:14:35  \n",
      "21    2020-02-29 15:07:31  \n",
      "22    2020-02-29 15:07:31  \n",
      "23    2020-02-29 15:07:31  \n",
      "24    2020-02-29 15:02:15  \n",
      "25    2020-02-29 15:02:15  \n",
      "26    2020-02-29 15:02:15  \n",
      "27    2020-02-29 15:02:15  \n",
      "28    2020-02-29 15:02:15  \n",
      "29    2020-02-29 14:58:13  \n",
      "...                   ...  \n",
      "37646 2020-02-29 16:37:14  \n",
      "37647 2020-02-29 16:38:21  \n",
      "37648 2020-02-29 16:38:28  \n",
      "37649 2020-02-29 16:38:17  \n",
      "37650 2020-02-29 16:38:11  \n",
      "37651 2020-02-29 16:37:12  \n",
      "37652 2020-02-29 16:38:12  \n",
      "37653 2020-02-29 16:38:02  \n",
      "37654 2020-02-29 16:37:48  \n",
      "37655 2020-02-29 16:37:20  \n",
      "37656 2020-02-29 16:38:14  \n",
      "37657 2020-02-29 16:37:56  \n",
      "37658 2020-02-29 16:38:08  \n",
      "37659 2020-02-29 16:37:59  \n",
      "37660 2020-02-29 16:37:22  \n",
      "37661 2020-02-29 16:37:52  \n",
      "37662 2020-02-29 16:37:28  \n",
      "37663 2020-02-29 16:37:33  \n",
      "37664 2020-02-29 16:37:16  \n",
      "37665 2020-02-29 16:38:03  \n",
      "37666 2020-02-29 16:38:03  \n",
      "37667 2020-02-29 16:37:35  \n",
      "37668 2020-02-29 16:38:02  \n",
      "37669 2020-02-29 16:37:13  \n",
      "37670 2020-02-29 16:37:49  \n",
      "37671 2020-02-29 16:37:12  \n",
      "37672 2020-02-29 16:37:08  \n",
      "37673 2020-02-29 16:37:38  \n",
      "37674 2020-02-29 16:37:39  \n",
      "37675 2020-02-29 16:38:14  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37676 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parke\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n",
      "C:\\Users\\parke\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    }
   ],
   "source": [
    "db['time'] = pd.to_datetime(db.article_date.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>article_date</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "      <th>author</th>\n",
       "      <th>twitter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37666</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2013/...</td>\n",
       "      <td>Wednesday, 31 July 2013</td>\n",
       "      <td>\\n1st Survival Summer Course\\n</td>\n",
       "      <td>A few weeks ago I had an idea, based on an app...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:38:03</td>\n",
       "      <td>2013-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37667</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2015/...</td>\n",
       "      <td>Wednesday, 4 November 2015</td>\n",
       "      <td>\\nWhat is the Difference Between Bio and Non-B...</td>\n",
       "      <td>When two forces in the universe secretively co...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:37:35</td>\n",
       "      <td>2015-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37668</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2013/...</td>\n",
       "      <td>Wednesday, 4 September 2013</td>\n",
       "      <td>\\nTerritorial Pissings - removing cat's urine ...</td>\n",
       "      <td>I recently moved to a new house to find out th...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:38:02</td>\n",
       "      <td>2013-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37669</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2018/...</td>\n",
       "      <td>Wednesday, 6 June 2018</td>\n",
       "      <td>\\nThe real survival rates to cancer - Part 2 o...</td>\n",
       "      <td>Following on the first part (see here) where t...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:37:13</td>\n",
       "      <td>2018-06-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37670</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2015/...</td>\n",
       "      <td>Wednesday, 7 January 2015</td>\n",
       "      <td>\\nGreat science news from a recent past - Part...</td>\n",
       "      <td>2014 was proficient in great science. News of ...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:37:49</td>\n",
       "      <td>2015-01-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37671</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2018/...</td>\n",
       "      <td>Wednesday, 7 November 2018</td>\n",
       "      <td>\\n'Amar a diferença' - a real case of a wonder...</td>\n",
       "      <td>When I moved from Portugal to England I was ta...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:37:12</td>\n",
       "      <td>2018-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37672</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2020/...</td>\n",
       "      <td>Wednesday, 8 January 2020</td>\n",
       "      <td>\\nOn the management of Paediatric Vesicoureter...</td>\n",
       "      <td>For any parent/carer dealing with a child diag...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:37:08</td>\n",
       "      <td>2020-01-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37673</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2015/...</td>\n",
       "      <td>Wednesday, 8 July 2015</td>\n",
       "      <td>\\nChris Dodd's opening welcome @ LeSpar AMR Wo...</td>\n",
       "      <td>If you haven't read the previous post please d...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:37:38</td>\n",
       "      <td>2015-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37674</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2015/...</td>\n",
       "      <td>Wednesday, 8 July 2015</td>\n",
       "      <td>\\nLESPAR Interdisciplinary Networking Workshop...</td>\n",
       "      <td>Yesterday I attended the LESPAR Interdisciplin...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:37:39</td>\n",
       "      <td>2015-07-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37675</th>\n",
       "      <td>http://thetoxicologisttoday.blogspot.com/2012/...</td>\n",
       "      <td>Wednesday, 9 May 2012</td>\n",
       "      <td>\\nDiscussing PhD careers outside of academia\\n</td>\n",
       "      <td>I'll be back to work on Monday, my rib is heal...</td>\n",
       "      <td>Pudget</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 16:38:14</td>\n",
       "      <td>2012-05-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article_url  \\\n",
       "37666  http://thetoxicologisttoday.blogspot.com/2013/...   \n",
       "37667  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
       "37668  http://thetoxicologisttoday.blogspot.com/2013/...   \n",
       "37669  http://thetoxicologisttoday.blogspot.com/2018/...   \n",
       "37670  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
       "37671  http://thetoxicologisttoday.blogspot.com/2018/...   \n",
       "37672  http://thetoxicologisttoday.blogspot.com/2020/...   \n",
       "37673  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
       "37674  http://thetoxicologisttoday.blogspot.com/2015/...   \n",
       "37675  http://thetoxicologisttoday.blogspot.com/2012/...   \n",
       "\n",
       "                      article_date  \\\n",
       "37666      Wednesday, 31 July 2013   \n",
       "37667   Wednesday, 4 November 2015   \n",
       "37668  Wednesday, 4 September 2013   \n",
       "37669       Wednesday, 6 June 2018   \n",
       "37670    Wednesday, 7 January 2015   \n",
       "37671   Wednesday, 7 November 2018   \n",
       "37672    Wednesday, 8 January 2020   \n",
       "37673       Wednesday, 8 July 2015   \n",
       "37674       Wednesday, 8 July 2015   \n",
       "37675        Wednesday, 9 May 2012   \n",
       "\n",
       "                                           article_title  \\\n",
       "37666                     \\n1st Survival Summer Course\\n   \n",
       "37667  \\nWhat is the Difference Between Bio and Non-B...   \n",
       "37668  \\nTerritorial Pissings - removing cat's urine ...   \n",
       "37669  \\nThe real survival rates to cancer - Part 2 o...   \n",
       "37670  \\nGreat science news from a recent past - Part...   \n",
       "37671  \\n'Amar a diferença' - a real case of a wonder...   \n",
       "37672  \\nOn the management of Paediatric Vesicoureter...   \n",
       "37673  \\nChris Dodd's opening welcome @ LeSpar AMR Wo...   \n",
       "37674  \\nLESPAR Interdisciplinary Networking Workshop...   \n",
       "37675     \\nDiscussing PhD careers outside of academia\\n   \n",
       "\n",
       "                                            article_text  author twitter  \\\n",
       "37666  A few weeks ago I had an idea, based on an app...  Pudget           \n",
       "37667  When two forces in the universe secretively co...  Pudget           \n",
       "37668  I recently moved to a new house to find out th...  Pudget           \n",
       "37669  Following on the first part (see here) where t...  Pudget           \n",
       "37670  2014 was proficient in great science. News of ...  Pudget           \n",
       "37671  When I moved from Portugal to England I was ta...  Pudget           \n",
       "37672  For any parent/carer dealing with a child diag...  Pudget           \n",
       "37673  If you haven't read the previous post please d...  Pudget           \n",
       "37674  Yesterday I attended the LESPAR Interdisciplin...  Pudget           \n",
       "37675  I'll be back to work on Monday, my rib is heal...  Pudget           \n",
       "\n",
       "                timestamp       time  \n",
       "37666 2020-02-29 16:38:03 2013-07-31  \n",
       "37667 2020-02-29 16:37:35 2015-11-04  \n",
       "37668 2020-02-29 16:38:02 2013-09-04  \n",
       "37669 2020-02-29 16:37:13 2018-06-06  \n",
       "37670 2020-02-29 16:37:49 2015-01-07  \n",
       "37671 2020-02-29 16:37:12 2018-11-07  \n",
       "37672 2020-02-29 16:37:08 2020-01-08  \n",
       "37673 2020-02-29 16:37:38 2015-07-08  \n",
       "37674 2020-02-29 16:37:39 2015-07-08  \n",
       "37675 2020-02-29 16:38:14 2012-05-09  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['year'] = pd.DatetimeIndex(db['time']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>article_date</th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_text</th>\n",
       "      <th>author</th>\n",
       "      <th>twitter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://thehill.com/blogs/twitter-room/other-n...</td>\n",
       "      <td>01/01/14 05:22 PM EST</td>\n",
       "      <td>Obama tweets: 'I signed the ACA for kids like ...</td>\n",
       "      <td>President Obama on Wednesday gave a shoutout t...</td>\n",
       "      <td>\\n    By Mario Trujillo -</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 15:19:44</td>\n",
       "      <td>2014-01-01 17:22:00</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://thehill.com/policy/healthcare/194204-o...</td>\n",
       "      <td>01/01/14 06:00 AM EST</td>\n",
       "      <td>O-Care coverage begins, presenting tests</td>\n",
       "      <td>Today is the day for ObamaCare.President Obam...</td>\n",
       "      <td>\\n    By Jonathan Easley -</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 15:19:44</td>\n",
       "      <td>2014-01-01 06:00:00</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://thehill.com/policy/healthcare/228294-o...</td>\n",
       "      <td>01/01/15 01:30 PM EST</td>\n",
       "      <td>ObamaCare's 2015 challenges</td>\n",
       "      <td>As ObamaCare enters its fifth year, there are ...</td>\n",
       "      <td>\\n    By Sarah Ferris -</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 15:14:35</td>\n",
       "      <td>2015-01-01 13:30:00</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://thehill.com/policy/healthcare/312343-d...</td>\n",
       "      <td>01/01/17 09:39 AM EST</td>\n",
       "      <td>Incoming Dem rep: Getting rid of ObamaCare wit...</td>\n",
       "      <td>Rep.-elect Lisa Blunt Rochester (D-Del.) said ...</td>\n",
       "      <td>Mallory Shelbourne</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 15:07:32</td>\n",
       "      <td>2017-01-01 09:39:00</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://thehill.com/policy/healthcare/311908-h...</td>\n",
       "      <td>01/01/17 10:30 AM EST</td>\n",
       "      <td>House, Senate headed for clash on Medicare</td>\n",
       "      <td>Senate and House Republicans are headed for a ...</td>\n",
       "      <td>Alexander Bolton</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-29 15:07:31</td>\n",
       "      <td>2017-01-01 10:30:00</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_url           article_date  \\\n",
       "0  https://thehill.com/blogs/twitter-room/other-n...  01/01/14 05:22 PM EST   \n",
       "1  https://thehill.com/policy/healthcare/194204-o...  01/01/14 06:00 AM EST   \n",
       "2  https://thehill.com/policy/healthcare/228294-o...  01/01/15 01:30 PM EST   \n",
       "3  https://thehill.com/policy/healthcare/312343-d...  01/01/17 09:39 AM EST   \n",
       "4  https://thehill.com/policy/healthcare/311908-h...  01/01/17 10:30 AM EST   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  Obama tweets: 'I signed the ACA for kids like ...   \n",
       "1           O-Care coverage begins, presenting tests   \n",
       "2                        ObamaCare's 2015 challenges   \n",
       "3  Incoming Dem rep: Getting rid of ObamaCare wit...   \n",
       "4         House, Senate headed for clash on Medicare   \n",
       "\n",
       "                                        article_text  \\\n",
       "0  President Obama on Wednesday gave a shoutout t...   \n",
       "1   Today is the day for ObamaCare.President Obam...   \n",
       "2  As ObamaCare enters its fifth year, there are ...   \n",
       "3  Rep.-elect Lisa Blunt Rochester (D-Del.) said ...   \n",
       "4  Senate and House Republicans are headed for a ...   \n",
       "\n",
       "                        author twitter           timestamp  \\\n",
       "0   \\n    By Mario Trujillo -          2020-02-29 15:19:44   \n",
       "1  \\n    By Jonathan Easley -          2020-02-29 15:19:44   \n",
       "2     \\n    By Sarah Ferris -          2020-02-29 15:14:35   \n",
       "3          Mallory Shelbourne          2020-02-29 15:07:32   \n",
       "4             Alexander Bolton         2020-02-29 15:07:31   \n",
       "\n",
       "                 time  year  \n",
       "0 2014-01-01 17:22:00  2014  \n",
       "1 2014-01-01 06:00:00  2014  \n",
       "2 2015-01-01 13:30:00  2015  \n",
       "3 2017-01-01 09:39:00  2017  \n",
       "4 2017-01-01 10:30:00  2017  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueyears, time_slices = np.unique(db['year'],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2006    1]\n",
      " [2007    5]\n",
      " [2008    1]\n",
      " [2010 2412]\n",
      " [2011 3232]\n",
      " [2012 3281]\n",
      " [2013 4033]\n",
      " [2014 4688]\n",
      " [2015 3952]\n",
      " [2016 3953]\n",
      " [2017 5126]\n",
      " [2018 3230]\n",
      " [2019 3081]\n",
      " [2020  681]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray((uniqueyears, time_slices)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    5    1 2412 3232 3281 4033 4688 3952 3953 5126 3230 3081  681]\n"
     ]
    }
   ],
   "source": [
    "print(time_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['president', 'obama', 'on', 'wednesday', 'gave', 'shoutout', 'to', 'the', 'then', 'year', 'old', 'boy', 'who', 'stood', 'beside', 'him', 'while', 'he', 'signed', 'his', 'signature', 'healthcare', 'law', 'in', 'in', 'tweet', 'obama', 'said', 'he', 'signed', 'the', 'law', 'for', 'kids', 'like', 'marcelas', 'owens', 'whose', 'mother', 'died', 'of', 'pulmonary', 'hypertension', 'in', 'part', 'because', 'she', 'went', 'without', 'healthcare', 'insurance', 'signed', 'the', 'aca', 'for', 'kids', 'like', 'marcelas', 'owens', 'he', 'lost', 'his', 'mom', 'bc', 'she', 'couldn', 'afford', 'coverage', 'today', 'millions', 'of', 'americans', 'finally', 'can', 'bomany', 'of', 'the', 'provisions', 'in', 'obamacare', 'including', 'coverage', 'and', 'subsidies', 'for', 'individual', 'who', 'purchased', 'insurance', 'on', 'the', 'online', 'healthcare', 'exchange', 'started', 'wednesday', 'the', 'tweet', 'was', 'signed', 'bo', 'signifying', 'that', 'obama', 'wrote', 'it', 'himself', 'in', 'an', 'interview', 'with', 'the', 'wall', 'street', 'journal', 'earlier', 'this', 'year', 'owens', 'now', 'in', 'high', 'school', 'said', 'the', 'passage', 'of', 'the', 'law', 'inspired', 'him', 'to', 'get', 'involved', 'in', 'other', 'advocacy', 'the', 'white', 'house', 'sent', 'out', 'number', 'of', 'tweets', 'on', 'wednesday', 'highlighting', 'positive', 'stories', 'about', 'healthcare', 'coverage', 'through', 'the', 'new', 'program', 'thanks', 'to', 'the', 'affordable', 'care', 'act', 'lucy', 'from', 'tx', 'is', 'saving', 'yr', 'on', 'her', 'insurance', 'premium', 'after', 'signing', 'up', 'at', 'http', 'co', 'weno', 'qcrz', 'the', 'administration', 'is', 'also', 'soliciting', 'stories', 'white', 'house', 'deputy', 'senior', 'adviser', 'david', 'simas', 'sent', 'an', 'email', 'to', 'supporters', 'asking', 'for', 'stories', 'about', 'coverage', 'on', 'wednesday', 'whether', 'you', 've', 'got', 'new', 'coverage', 'today', 'know', 'someone', 'who', 'does', 'or', 'simply', 'want', 'to', 'help', 'get', 'the', 'word', 'out', 'about', 'the', 'importance', 'of', 'getting', 'covered', 'we', 'want', 'to', 'hear', 'from', 'you', 'he', 'said', 'in', 'the', 'message']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        #sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        #sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        #sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data = db.article_text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-9f4d90623171>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Build the bigram and trigram models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# higher threshold fewer phrases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mbigram_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphrases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhraser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrigram_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphrases\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhraser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, min_count, threshold, max_vocab_size, delimiter, progress_per, scoring, common_terms)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36madd_vocab\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;31m# counts collected in previous learn_vocab runs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         min_reduce, vocab, total_words = self.learn_vocab(\n\u001b[1;32m--> 566\u001b[1;33m             sentences, self.max_vocab_size, self.delimiter, self.progress_per, self.common_terms)\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_word_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36mlearn_vocab\u001b[1;34m(sentences, max_vocab_size, delimiter, progress_per, common_terms)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[0mmin_reduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msentence_no\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprogress_per\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m                 logger.info(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \"\"\"\n\u001b[1;32m--> 682\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_sentence2token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36m_sentence2token\u001b[1;34m(phrase_class, sentence)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[0mnew_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36manalyze_sentence\u001b[1;34m(self, sentence, threshold, common_terms, scorer)\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mwordb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[0mcomponents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                     \u001b[0mscorer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m                 )\n\u001b[0;32m    191\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36mscore_item\u001b[1;34m(self, worda, wordb, components, scorer)\u001b[0m\n\u001b[0;32m    147\u001b[0m                     \u001b[0mworda_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mworda\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                     \u001b[0mwordb_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                     bigram_count=float(vocab[bigram]))\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#!python -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(r'C:\\Users\\parke\\Anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.5', disable=['parser', 'ner'])\n",
    "    # nlp = spacy.load(r'C:\\Users\\parke\\Anaconda3\\Lib\\site-packages\\en_core_web_lg\\en_core_web_lg-2.2.5', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parke\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py:1474: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d892e761144b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                                  \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0mtime_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_slices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                  num_topics = 30)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, time_slice, id2word, alphas, num_topics, initialize, sstats, lda_model, obs_variance, chain_variance, passes, random_state, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;31m# fit DTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_lda_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_inference_max_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mem_min_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mem_max_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_ldaseq_ss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_chain_variance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_obs_variance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_suffstats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py\u001b[0m in \u001b[0;36mfit_lda_seq\u001b[1;34m(self, corpus, lda_inference_max_iter, em_min_iter, em_max_iter, chunksize)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;31m# seq model and find the evidence lower bound. This is the E - Step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0mbound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlda_seq_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_suffstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlhoods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_inference_max_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgammas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py\u001b[0m in \u001b[0;36mlda_seq_infer\u001b[1;34m(self, corpus, topic_suffstats, gammas, lhoods, iter_, lda_inference_max_iter, chunksize)\u001b[0m\n\u001b[0;32m    351\u001b[0m             bound, gammas = self.inferDTMseq(\n\u001b[0;32m    352\u001b[0m                 \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_suffstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlhoods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                 \u001b[0mldapost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_inference_max_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m             )\n\u001b[0;32m    355\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"DIM\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py\u001b[0m in \u001b[0;36minferDTMseq\u001b[1;34m(self, corpus, topic_suffstats, gammas, lhoods, lda, ldapost, iter_, bound, lda_inference_max_iter, chunksize)\u001b[0m\n\u001b[0;32m    424\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0miter_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     doc_lhood = LdaPost.fit_lda_post(\n\u001b[1;32m--> 426\u001b[1;33m                         \u001b[0mldapost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda_inference_max_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlda_inference_max_iter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m                     )\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py\u001b[0m in \u001b[0;36mfit_lda_post\u001b[1;34m(self, doc_number, time, ldaseq, LDA_INFERENCE_CONVERGED, lda_inference_max_iter, g, g3_matrix, g4_matrix, g5_matrix)\u001b[0m\n\u001b[0;32m   1486\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_phi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_phi_fixed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msslm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg3_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg4_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg5_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1488\u001b[1;33m             \u001b[0mlhood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_lda_lhood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1489\u001b[0m             \u001b[0mconverged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlhood_old\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlhood\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlhood_old\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py\u001b[0m in \u001b[0;36mcompute_lda_lhood\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1405\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m                     \u001b[0mlhood_term\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m                         \u001b[0mcount\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me_log_theta_k\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_phi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m                 \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlhood\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlhood_term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ldaseq = ldaseqmodel.LdaSeqModel(corpus = corpus, \n",
    "                                 id2word = id2word, \n",
    "                                 time_slice = [2,2,2,2],\n",
    "                                 num_topics = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "doc_topic, topic_term, doc_lengths, term_frequency, vocab = ldaseq.dtm_vis(time=8, corpus=corpus)\n",
    "vis_dtm = pyLDAvis.prepare(topic_term_dists=topic_term, doc_topic_dists=doc_topic, doc_lengths=doc_lengths, vocab=vocab, term_frequency=term_frequency)\n",
    "pyLDAvis.display(vis_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DATE   PROJECT_ID                                           ABSTRACT\n",
      "0   2014      589214  Objective(s): 1: Determine the nutrients in po...\n",
      "1   2015      673465  This project has four broad objectives, linked...\n",
      "2   2016      917061  Conservation needs to put people back into the...\n",
      "3   2017      931981  AbstractThe goal of this program is to develop...\n",
      "4   2018     1005000  DESCRIPTION (provided by applicant): There is ...\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueyears2, time_slices2 = np.unique(df['DATE '],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2014    1]\n",
      " [2015    1]\n",
      " [2016    1]\n",
      " [2017    1]\n",
      " [2018    1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray((uniqueyears2, time_slices2)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['objective', 'determine', 'the', 'nutrients', 'in', 'pollen', 'that', 'promote', 'worker', 'longevity', 'determine', 'the', 'effects', 'of', 'pollen', 'mixtures', 'on', 'worker', 'protein', 'and', 'lipid', 'storesand', 'longevity', 'characterize', 'the', 'chemical', 'composition', 'of', 'pollen', 'mixtures', 'that', 'optimize', 'workerprotein', 'and', 'lipid', 'stores', 'and', 'longevity', 'determine', 'the', 'effects', 'of', 'undigested', 'saccharides', 'in', 'high', 'fructose', 'corn', 'syrup', 'hfcs', 'on', 'worker', 'physiology', 'and', 'longevity', 'identify', 'the', 'saccharides', 'in', 'hfcs', 'determine', 'the', 'effect', 'of', 'saccharides', 'in', 'hfcs', 'on', 'worker', 'physiology', 'and', 'longevity', 'evaluate', 'the', 'effects', 'of', 'supplemental', 'feeding', 'on', 'varroa', 'tolerance', 'queenproduction', 'and', 'foraging', 'activity', 'of', 'honey', 'bee', 'colonies', 'modify', 'the', 'megabee', 'diet', 'by', 'adding', 'chemical', 'components', 'that', 'were', 'identified', 'inthe', 'pollen', 'mixture', 'analysis', 'determine', 'the', 'effects', 'of', 'nutrition', 'on', 'varroa', 'infestation', 'and', 'reproduction', 'inworker', 'and', 'drone', 'cells', 'determine', 'the', 'role', 'of', 'nutrition', 'on', 'queen', 'production', 'and', 'reproductive', 'potential', 'evaluate', 'the', 'effects', 'of', 'supplemental', 'protein', 'feeding', 'on', 'the', 'foraging', 'rates', 'ofhoney', 'bee', 'colonies', 'improving', 'honey', 'bee', 'immune', 'response', 'to', 'ccd', 'by', 'determining', 'the', 'role', 'of', 'in', 'bee', 'nutrition', 'develop', 'ipm', 'tools', 'soft', 'miticides', 'that', 'are', 'non', 'toxic', 'to', 'workers', 'andqueens', 'traps', 'and', 'exclusion', 'devices', 'and', 'methodologies', 'for', 'control', 'of', 'key', 'pests', 'especially', 'varroa', 'mites', 'and', 'miticide', 'resistance', 'management', 'programs', 'to', 'preserveuseful', 'chemical', 'options', 'determine', 'the', 'impact', 'of', 'the', 'small', 'hive', 'beetle', 'on', 'colony', 'development', 'andlongevity', 'and', 'develop', 'management', 'systems', 'for', 'controlling', 'the', 'beetle', 'in', 'hives', 'including', 'use', 'of', 'antifeedants', 'for', 'protection', 'of', 'protein', 'supplements', 'from', 'small', 'hivebeetle', 'damage', 'develop', 'effective', 'control', 'programs', 'for', 'management', 'of', 'small', 'hivebeetle', 'in', 'bee', 'hives', 'with', 'the', 'goal', 'to', 'prevent', 'contamination', 'of', 'bee', 'products', 'approach', 'nutritional', 'value', 'will', 'be', 'evaluated', 'by', 'measuring', 'protein', 'and', 'lipid', 'levels', 'and', 'onbee', 'longevity', 'the', 'chemical', 'composition', 'of', 'pollens', 'that', 'are', 'more', 'nutritious', 'thanmegabee', 'will', 'be', 'determined', 'determine', 'the', 'effects', 'of', 'high', 'fructose', 'corn', 'syrup', 'containing', 'higher', 'saccharideson', 'honey', 'bee', 'longevity', 'determine', 'the', 'effects', 'of', 'improved', 'nutrition', 'of', 'the', 'longevity', 'of', 'bees', 'parasitized', 'by', 'varroa', 'the', 'reproductive', 'potential', 'of', 'queens', 'and', 'foraging', 'activity', 'of', 'colonies', 'used', 'for', 'pollination']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        #sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        #sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        #sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data2 = df.ABSTRACT.values.tolist()\n",
    "data_words2 = list(sent_to_words(data2))\n",
    "print(data_words2[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words2, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words2], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#!python -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(r'C:\\Users\\parke\\Anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.5', disable=['parser', 'ner'])\n",
    "    # nlp = spacy.load(r'C:\\Users\\parke\\Anaconda3\\Lib\\site-packages\\en_core_web_lg\\en_core_web_lg-2.2.5', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready2 = process_words(data_words2)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word2 = corpora.Dictionary(data_ready2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "corpus2 = [id2word2.doc2bow(text) for text in data_ready2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parke\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldaseqmodel.py:293: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  convergence = np.fabs((bound - old_bound) / old_bound)\n"
     ]
    }
   ],
   "source": [
    "ldaseq = ldaseqmodel.LdaSeqModel(corpus = corpus2, \n",
    "                                 id2word = id2word2, \n",
    "                                 time_slice = time_slices2,\n",
    "                                 num_topics = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('resource', 0.036058831910797534),\n",
       "  ('stewardship', 0.033113556223825814),\n",
       "  ('land', 0.024241697761567833),\n",
       "  ('management', 0.024200476140575616),\n",
       "  ('people', 0.021302246511092734),\n",
       "  ('urban', 0.021302244774478073),\n",
       "  ('practice', 0.01837315748024876),\n",
       "  ('natural', 0.01836674513199405),\n",
       "  ('range', 0.015463240505383884),\n",
       "  ('research', 0.015430549544201385),\n",
       "  ('steward', 0.012569958507057843),\n",
       "  ('benefit', 0.012569949878311806),\n",
       "  ('work', 0.012569913905180415),\n",
       "  ('landscape', 0.012569910683553214),\n",
       "  ('public', 0.01256990729265597),\n",
       "  ('forest', 0.012569905587804586),\n",
       "  ('organization', 0.012569898828702804),\n",
       "  ('rural', 0.01256989714518662),\n",
       "  ('private', 0.012569893886326759),\n",
       "  ('ecological', 0.012569869347346049)],\n",
       " [('resource', 0.03610944457064613),\n",
       "  ('stewardship', 0.03316113706910026),\n",
       "  ('land', 0.02426535252017068),\n",
       "  ('management', 0.02422141209564925),\n",
       "  ('people', 0.021320381330698437),\n",
       "  ('urban', 0.021320379604742975),\n",
       "  ('practice', 0.01838592559625764),\n",
       "  ('natural', 0.01837954106044524),\n",
       "  ('range', 0.01546972690273585),\n",
       "  ('research', 0.015436966937150885),\n",
       "  ('steward', 0.012573611289436909),\n",
       "  ('benefit', 0.012573605831181708),\n",
       "  ('work', 0.01257359916913158),\n",
       "  ('landscape', 0.012573594518343629),\n",
       "  ('public', 0.012573590500787319),\n",
       "  ('forest', 0.012573588978168303),\n",
       "  ('rural', 0.01257358558530664),\n",
       "  ('private', 0.012573580289581682),\n",
       "  ('organization', 0.012573579861779382),\n",
       "  ('ecological', 0.01257357705841248)],\n",
       " [('resource', 0.03611188429374994),\n",
       "  ('stewardship', 0.03315109281470261),\n",
       "  ('land', 0.024270960372200907),\n",
       "  ('management', 0.024221165907640823),\n",
       "  ('people', 0.021324230619365416),\n",
       "  ('urban', 0.021324228869708046),\n",
       "  ('practice', 0.018389526806680153),\n",
       "  ('natural', 0.018383073448308817),\n",
       "  ('range', 0.015469638034216075),\n",
       "  ('research', 0.015436831192865652),\n",
       "  ('steward', 0.012575500485497744),\n",
       "  ('benefit', 0.012575490819306905),\n",
       "  ('work', 0.012575445040133547),\n",
       "  ('landscape', 0.012575442262834434),\n",
       "  ('public', 0.012575439024185016),\n",
       "  ('forest', 0.012575437198852103),\n",
       "  ('organization', 0.01257543119488927),\n",
       "  ('rural', 0.012575427134972488),\n",
       "  ('private', 0.012575423692215983),\n",
       "  ('ecological', 0.012575388011019144)],\n",
       " [('resource', 0.036114107944709664),\n",
       "  ('stewardship', 0.03314635081950308),\n",
       "  ('land', 0.02427506579335422),\n",
       "  ('management', 0.024220020573133536),\n",
       "  ('people', 0.021327539436522533),\n",
       "  ('urban', 0.021327537688303984),\n",
       "  ('practice', 0.018392648208185657),\n",
       "  ('natural', 0.018386196565068434),\n",
       "  ('range', 0.015469216450778677),\n",
       "  ('research', 0.015436370931686342),\n",
       "  ('steward', 0.012577042459672802),\n",
       "  ('benefit', 0.012577035677369786),\n",
       "  ('work', 0.012577017846124576),\n",
       "  ('landscape', 0.012577013704873484),\n",
       "  ('public', 0.012577009940433351),\n",
       "  ('forest', 0.012577008412197232),\n",
       "  ('rural', 0.012577002952045483),\n",
       "  ('organization', 0.012577000077779715),\n",
       "  ('private', 0.012576999622930369),\n",
       "  ('ecological', 0.01257699406579307)],\n",
       " [('resource', 0.036118217786144415),\n",
       "  ('stewardship', 0.03314876274767862),\n",
       "  ('land', 0.024278190323042473),\n",
       "  ('management', 0.024219245085888064),\n",
       "  ('people', 0.021329882760972845),\n",
       "  ('urban', 0.021329881011357663),\n",
       "  ('practice', 0.018394548186488384),\n",
       "  ('natural', 0.01838809215578391),\n",
       "  ('range', 0.015468646836691725),\n",
       "  ('research', 0.015435768671992981),\n",
       "  ('steward', 0.012577931445610537),\n",
       "  ('benefit', 0.012577923574247479),\n",
       "  ('work', 0.01257789459842838),\n",
       "  ('landscape', 0.012577890984652333),\n",
       "  ('public', 0.012577887375739411),\n",
       "  ('forest', 0.01257788565540651),\n",
       "  ('rural', 0.012577878468727897),\n",
       "  ('organization', 0.012577878262898011),\n",
       "  ('private', 0.012577873941000095),\n",
       "  ('ecological', 0.012577851099117634)]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaseq.print_topic_times(topic=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parke\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2044422641487078725312456246\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2044422641487078725312456246_data = {\"mdsDat\": {\"x\": [0.24113179230068726, -0.24113179230068726], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [70.27032367056034, 29.72967632943966]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [4.0, 4.0, 3.0, 9.0, 2.0, 2.0, 2.0, 5.0, 2.0, 6.0, 6.0, 2.0, 6.0, 5.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 9.456620603583145, 6.628849997653684, 6.086515462218697, 5.580862052536421, 5.542673840297132, 4.998211779441291, 4.450351757024356, 4.009574615295986, 3.898716628496512, 3.4777354397507456, 3.477735532339543, 3.477735532818834, 3.4402346945417737, 3.346254168155137, 3.3462542111164795, 3.346252064094882, 6.714534550398634, 2.8818578013542933, 2.881857550320311, 2.808369668876504, 2.817426488517066, 2.7960002953721466, 2.795533082257706, 2.2924138424755265, 2.2924138437824753, 2.292419317858048, 2.2924202348721026, 2.2924204220169027, 2.292420643920113, 2.2924223450792995, 3.04675635916411, 2.5067945172352544, 4.713550754254995, 5.070474826406034, 2.8114292132027434, 2.8047677378404265, 2.2924251940082265, 2.2988491381515717, 4.716876566712996, 4.331603358064592, 3.1710704437610597, 2.786559133004778, 2.786558905837214, 2.4034033101655226, 1.6442776763157212, 1.644271847679749, 1.6442772548928117, 1.6442685806266317, 1.6442768113275907, 1.6442754839312814, 1.6442765883149992, 1.6442835107129914, 1.6442685422576881, 1.6442823819817534, 1.6442750576379466, 1.644275704153003, 1.2705993970288487, 1.2705980780381154, 1.2705968250676607, 2.0227553950091126, 2.4025645071977766, 1.643883234195216, 1.6438851981023623, 0.8467177746598118, 0.846717766723493, 0.8467178372116143, 0.8467178367837296, 0.84671783521121, 1.6429501255211714, 3.1656782197816167, 2.0184790715517553, 1.638025166592487, 1.2662175823792152, 1.6294149354544694], \"Term\": [\"resource\", \"stewardship\", \"land\", \"determine\", \"people\", \"urban\", \"practice\", \"management\", \"natural\", \"light\", \"impact\", \"range\", \"visual\", \"type\", \"adaptation\", \"work\", \"landscape\", \"ecological\", \"individual\", \"public\", \"rural\", \"steward\", \"forest\", \"cultural\", \"benefit\", \"private\", \"organization\", \"social\", \"health\", \"cycle\", \"determine\", \"light\", \"visual\", \"type\", \"adaptation\", \"cycle\", \"circadian\", \"develop\", \"diverse\", \"stove\", \"pollen\", \"worker\", \"small\", \"signal\", \"rgc\", \"response\", \"impact\", \"chemical\", \"nutrition\", \"molecule\", \"level\", \"parallel\", \"process\", \"climate\", \"adoption\", \"high\", \"control\", \"saccharide\", \"forage\", \"mixture\", \"understand\", \"objective\", \"effect\", \"different\", \"provide\", \"information\", \"protein\", \"management\", \"resource\", \"stewardship\", \"land\", \"people\", \"urban\", \"practice\", \"work\", \"ecological\", \"landscape\", \"individual\", \"public\", \"rural\", \"forest\", \"steward\", \"cultural\", \"benefit\", \"private\", \"organization\", \"partner\", \"knowledge\", \"conduct\", \"range\", \"natural\", \"social\", \"health\", \"stakeholder\", \"policy\", \"issue\", \"business\", \"gatherer\", \"activity\", \"management\", \"research\", \"area\", \"condition\", \"different\"], \"Total\": [4.0, 4.0, 3.0, 9.0, 2.0, 2.0, 2.0, 5.0, 2.0, 6.0, 6.0, 2.0, 6.0, 5.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 5.0, 9.462488125880062, 6.634793961169838, 6.092458108565551, 5.586748324170752, 5.548593075293286, 5.004124801706685, 4.4562591990725595, 4.015397131896989, 3.9045994790297165, 3.4835356302485594, 3.483559823894381, 3.483559948286187, 3.446047806473866, 3.352137458946458, 3.352139888691641, 3.3521517788740787, 6.726831167679351, 2.887681017476819, 2.887681158193431, 2.8142043260640426, 2.8232820497848765, 2.801861669412033, 2.8014129475765093, 2.298214030324198, 2.2982140354326073, 2.298231595040798, 2.298234076476883, 2.298234572617464, 2.2982351567002577, 2.2982394976447154, 3.0590503997900553, 2.5190469185306927, 5.560268680884108, 6.699889761860503, 3.6581471445759552, 3.651485660165013, 2.2982463294793622, 5.464527357933188, 4.91075615550916, 4.525058357756838, 3.361787603930779, 2.9771053029439387, 2.977175820780639, 2.592644662757074, 1.832067280430364, 1.8323002490106357, 1.832313491807893, 1.8324263440122408, 1.8325067778368567, 1.8325529931659656, 1.8325665646430174, 1.8325808373087786, 1.8326157118853412, 1.8327149997599292, 1.8328437182413888, 1.8330352330976856, 1.4574505501365487, 1.4578986088416148, 1.4583242622308117, 2.344076970237589, 2.856517452902712, 1.9646218552466763, 1.9650885152549906, 1.0330842348879994, 1.0331251846732157, 1.0331590296657986, 1.0331612356856548, 1.033169342721074, 2.27074690716051, 5.464527357933188, 3.715491215167824, 3.885834989779293, 2.960255929934789, 6.699889761860503], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3522, 0.3519, 0.3518, 0.3518, 0.3518, 0.3516, 0.3515, 0.3514, 0.3513, 0.3512, 0.3511, 0.3511, 0.3511, 0.3511, 0.3511, 0.3511, 0.351, 0.3508, 0.3508, 0.3507, 0.3507, 0.3507, 0.3507, 0.3503, 0.3503, 0.3503, 0.3503, 0.3503, 0.3503, 0.3503, 0.3488, 0.3479, 0.1876, 0.0742, 0.0896, 0.089, 0.3503, -0.513, 1.1727, 1.1693, 1.1546, 1.1469, 1.1469, 1.1372, 1.1049, 1.1047, 1.1047, 1.1047, 1.1046, 1.1046, 1.1046, 1.1046, 1.1046, 1.1045, 1.1045, 1.1044, 1.0758, 1.0755, 1.0752, 1.0656, 1.04, 1.0348, 1.0345, 1.0141, 1.014, 1.014, 1.014, 1.014, 0.8894, 0.6671, 0.6029, 0.3492, 0.3638, -0.2008], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.4872, -3.8425, -3.9279, -4.0146, -4.0215, -4.1249, -4.241, -4.3453, -4.3733, -4.4876, -4.4876, -4.4876, -4.4984, -4.5261, -4.5261, -4.5261, -3.8297, -4.6755, -4.6755, -4.7013, -4.6981, -4.7058, -4.7059, -4.9043, -4.9043, -4.9043, -4.9043, -4.9043, -4.9043, -4.9043, -4.6199, -4.8149, -4.1835, -4.1105, -4.7003, -4.7026, -4.9043, -4.9015, -3.3226, -3.4078, -3.7197, -3.8489, -3.8489, -3.9969, -4.3764, -4.3765, -4.3764, -4.3765, -4.3764, -4.3765, -4.3764, -4.3764, -4.3765, -4.3764, -4.3765, -4.3765, -4.6343, -4.6343, -4.6343, -4.1693, -3.9972, -4.3767, -4.3767, -5.0401, -5.0401, -5.0401, -5.0401, -5.0401, -4.3773, -3.7214, -4.1714, -4.3803, -4.6377, -4.3855]}, \"token.table\": {\"Topic\": [1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1], \"Freq\": [0.44038373314376333, 0.8807674662875267, 1.0813552045682957, 0.8702409650124373, 0.5146898942596622, 0.5146898942596622, 1.0912771490722692, 0.967903135986662, 1.0388959105397737, 0.8976138553234254, 0.8702409669467859, 0.6756172599049765, 0.3378086299524882, 0.6857185510102473, 0.8702333763434289, 1.0913362725360785, 0.9991757196572559, 0.9511240468968041, 0.996165477189123, 0.7462809356152066, 0.2985123742460826, 1.0244328570657881, 1.091524165365319, 0.8992371208948444, 0.1798474241789689, 0.8702329673137298, 1.0913655408689609, 0.9678955410797272, 1.0177658586236658, 0.8702343159478217, 1.0406088432296552, 1.0914490541654425, 0.8215833989786031, 0.27386113299286774, 0.967905202670953, 0.6859187558965834, 0.8923823731434557, 1.09151627652245, 1.0625930909838033, 1.0550440663218077, 0.36599688664683466, 0.548995329970252, 0.8702313236064573, 1.0660206766847709, 0.700153257585616, 1.0388958599144087, 1.1909266071748426, 1.0910865017145126, 1.07071667125863, 0.6861296253971086, 1.0076902543666901, 0.9679369110688233, 0.8611880236482363, 0.7714130782091662, 1.0912005099480044, 1.0708881754099435, 0.8702287367312249, 0.8200872959547815, 0.27336243198492716, 1.0914011474276002, 0.8532143037083315, 0.5382868332013167, 0.5382868332013167, 1.0181731370210108, 0.8949475435171496, 0.8949507179340648, 1.091373623277736, 0.8702331884783179, 0.8949513666253021, 0.870562501879427, 1.0180076102985638, 0.9679752785196784, 1.0913570410007578, 0.8839665002647348, 0.8611940047204116, 1.0739699825104594, 0.9806964933320131, 1.0076663860629422, 0.9848241699954307, 1.0916629653088874, 0.8611879928967249], \"Term\": [\"activity\", \"activity\", \"adaptation\", \"adoption\", \"area\", \"area\", \"benefit\", \"business\", \"chemical\", \"circadian\", \"climate\", \"condition\", \"condition\", \"conduct\", \"control\", \"cultural\", \"cycle\", \"determine\", \"develop\", \"different\", \"different\", \"diverse\", \"ecological\", \"effect\", \"effect\", \"forage\", \"forest\", \"gatherer\", \"health\", \"high\", \"impact\", \"individual\", \"information\", \"information\", \"issue\", \"knowledge\", \"land\", \"landscape\", \"level\", \"light\", \"management\", \"management\", \"mixture\", \"molecule\", \"natural\", \"nutrition\", \"objective\", \"organization\", \"parallel\", \"partner\", \"people\", \"policy\", \"pollen\", \"practice\", \"private\", \"process\", \"protein\", \"provide\", \"provide\", \"public\", \"range\", \"research\", \"research\", \"resource\", \"response\", \"rgc\", \"rural\", \"saccharide\", \"signal\", \"small\", \"social\", \"stakeholder\", \"steward\", \"stewardship\", \"stove\", \"type\", \"understand\", \"urban\", \"visual\", \"work\", \"worker\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2044422641487078725312456246\", ldavis_el2044422641487078725312456246_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2044422641487078725312456246\", ldavis_el2044422641487078725312456246_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2044422641487078725312456246\", ldavis_el2044422641487078725312456246_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "doc_topic, topic_term, doc_lengths, term_frequency, vocab = ldaseq.dtm_vis(time=0, corpus=corpus2)\n",
    "vis_dtm = pyLDAvis.prepare(topic_term_dists=topic_term, doc_topic_dists=doc_topic, doc_lengths=doc_lengths, vocab=vocab, term_frequency=term_frequency)\n",
    "pyLDAvis.display(vis_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
